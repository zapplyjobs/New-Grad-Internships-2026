{
  "url": "https://apply.careers.microsoft.com/careers/job/1970393556641879",
  "data": {
    "success": true,
    "platform": "generic",
    "description": "Research Interns put inquiry and theory into practice. Alongside fellow doctoral candidates and some of the world&#39;s best researchers, Research Interns learn, collaborate, and network for life. Research Interns not only advance their own careers, but they also contribute to exciting research and development strides. During the 12-week internship, Research Interns are paired with mentors and expected to collaborate with other Research Interns and researchers, present findings, and contribute to the vibrant life of the community. Research internships are available in all areas of research, and are offered year-round, though they typically begin in the summer. As a Research Intern, you will be at the forefront of hardware/software co-design and have a direct impact in answering critical questions around designing an optimized AI system and evaluating real-world impact on the Azure&#39;s supporting hyperscale infrastructure. This role will evaluate opportunities to co-optimize central processing unit (CPU), graphics processing unit (GPU) and networking infrastructure for the Maia accelerator ecosystem. You will be expected to identify system stress points, propose novel architectural ideas, and create methodologies using a combination of workload characterization, modeling and benchmarking to evaluate their effectiveness. Accepted or currently enrolled in a PhD program in Computer Science or related STEM field. At least 1 year of experience with performance analysis tools and methodologies, optimization and modeling. In addition to the qualifications below, you&#39;ll need to submit a minimum of two reference letters for this position as well as a cover letter and any relevant work or research samples. After you submit your application, a request for letters may be sent to your list of references on your behalf. Note that reference letters cannot be requested until after you have submitted your application, and furthermore, that they might not be automatically requested for all candidates. You may wish to alert your letter writers in advance, so they will be ready to submit your letter. Proficiency with frameworks such as PyTorch, SGLang, Dynamo, and AI accelerator programming models/compilers such as CUDA and Triton. Deep understanding of GPU and AI architectures including memory hierarchies, compute-communication interplay, kernel scheduling and interconnect properties. Familiarity with CPU/server architectures including understanding of PCIe topologies and accelerator/NIC/peripheral demand. Solid understanding of CPU involvement in dispatching, scheduling and orchestration of input data pipelines to AI accelerators. Hands-on experience with benchmarking, profiling, identifying perf bottlenecks and performance analysis and optimization, including trace generation, event monitoring and instrumentation. Familiarity with roofline performance modeling, detailed performance simulations and awareness of speed vs accuracy tradeoffs in various performance modeling methodologies. Ability to apply the appropriate performance analysis methodology including devising new or combinatorial approaches in evaluating complex system architecture what-if scenarios. Solid verbal and written communication skills.",
    "requirements": null,
    "responsibilities": null,
    "metadata": {
      "source": "generic",
      "extractionMethod": "json-ld",
      "url": "https://apply.careers.microsoft.com/careers/job/1970393556641879",
      "fetchedAt": "2025-12-19T23:16:47.696Z"
    }
  },
  "cachedAt": 1766186207704
}